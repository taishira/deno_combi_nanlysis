{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>original</th>\n",
       "      <th>重複</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>import</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, original, 重複]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = \"\"\"\n",
    "ID,氏名,生年月日,電話番号\n",
    "00001,山田太郎,1980-01-01,09012345678\n",
    "00002,佐藤花子,1982-11-11,09087654321\n",
    "00001,山田太郎,1980-01-01,09012345678\n",
    "00003,田中桃子,1975-12-22,09012346666\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv('deno.land_dependencies_import_https.csv')\n",
    "\n",
    "# 重複のみ抽出\n",
    "df1 = df[df.duplicated(keep=False)]\n",
    "\n",
    "# IDでグループ化、indexをリスト化\n",
    "df2 = df1.reset_index().groupby(\"import\")[\"index\"].apply(list)\n",
    "df2.name = \"重複\"\n",
    "\n",
    "# 重複の最初のみ抽出\n",
    "df3 = df1.groupby(by=\"import\").first()\n",
    "\n",
    "# indexのリストを結合\n",
    "df4 = pd.merge(df3, df2, on=\"import\")\n",
    "\n",
    "\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('deno.land_dependencies_import_https.csv')\n",
    "\n",
    "#df.groupby(\"import\").groups #importフィールドにある重複した項目とそのインデックスIDの表示\n",
    "#df.groupby(\"import\").size() #importフィールドそれぞれの項目の個数の表示\n",
    "\n",
    "#df.groupby(\"import\").apply(print)\n",
    "#df.groupby(\"import\").sum()\n",
    "#df_a.groupby(\"import\").groups\n",
    "#df_a=df.sort_values(by='import')\n",
    "\n",
    "\n",
    "#from collections import Counter\n",
    "#c = Counter(df[\"import\"])\n",
    "#print(c)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "df = pd.read_csv('deno.land_dependencies_import_https.csv')\n",
    "\n",
    "df2 = df['import'].str.contains('@')\n",
    "\n",
    "df3 = df[df2]\n",
    "\n",
    "df_sort = df3.sort_values(by='import')\n",
    "\n",
    "def search_moji(text, left_str, right_str):\n",
    "    new_text = []\n",
    "    number = 0\n",
    "    moji_find = 0\n",
    "    for line in text:\n",
    "        number += 1\n",
    "        if line.find(left_str) != -1:\n",
    "            moji_find = 1\n",
    "        elif moji_find != 1:\n",
    "            new_text.append(line)\n",
    "        elif line.find(right_str) != -1:\n",
    "            moji_find = 0\n",
    "            new_text.append(line)\n",
    "    text = \"\".join(new_text)\n",
    "    return text\n",
    "\n",
    "#return(text,hajime_num,owari_num)\n",
    "\n",
    "#df_sort\n",
    "list_http_not_v = []\n",
    "\n",
    "for i in df_sort[\"import\"]:\n",
    "    not_v = search_moji(i,\"@\",\"/\")\n",
    "    list_http_not_v.append(not_v)\n",
    "    \n",
    "    \n",
    "#pprint.pprint(list_http_not_v)\n",
    "\n",
    "#from collections import Counter\n",
    "#c = Counter(df_sort[\"import\"])\n",
    "#print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "はじめは https://deno.land/x/gokv\n",
      "おわりは /mod.ts\n",
      "@あり\n",
      "------------------\n",
      "はじめは https://deno.land/std\n",
      "おわりは /http/http_status.ts\n",
      "@あり\n",
      "------------------\n",
      "はじめは https://deno.land/std\n",
      "おわりは /http/http_status.ts\n",
      "@あり\n",
      "------------------\n",
      "はじめは https://deno.land\n",
      "おわりは a1/server.ts\n",
      "@なし\n",
      "------------------\n",
      "はじめは https://deno.land/x/\n",
      "おわりは /a1/server.ts\n",
      "@あり\n",
      "------------------\n",
      "[('https://deno.land/x/', 'https://deno.land/x/', '/a1/server.ts')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def search_at(text, left_str, right_str):\n",
    "    global start\n",
    "    global end\n",
    "    global new_text\n",
    "    end_list = []\n",
    "    new_text_list = []\n",
    "    start_list = []\n",
    "    a= []\n",
    "    moji_find = 0\n",
    "    for line in text:\n",
    "        if line.find(left_str) == 0: # @みつけたら\n",
    "            new_text = \"\".join(new_text_list)\n",
    "            start = new_text\n",
    "            moji_find = 1\n",
    "        elif moji_find != 1: # @~/以外の文字列を追加し続ける\n",
    "            new_text_list.append(line)\n",
    "            end_list.append(line)\n",
    "        elif line.find(right_str) == 0: # /みつけたら\n",
    "            moji_find = 0\n",
    "            new_text_list.append(line)\n",
    "            end_list = []\n",
    "            end_list.append(\"/\")\n",
    "    end =  \"\".join(end_list)\n",
    "    text = \"\".join(new_text) #リストをstr形式に変換する\n",
    "    return text , start , end\n",
    "\n",
    "   \n",
    "            \n",
    "            \n",
    "#------------------------------------------------------------\n",
    "https_list_a = ['https://deno.land/x/gokv@0.0.12/mod.ts',\n",
    " 'https://deno.land/std@0.130.0/http/http_status.ts',\n",
    "'https://deno.land/std@0.140.0/http/http_status.ts',\n",
    " 'https://deno.land/x/a1/server.ts',\n",
    "'https://deno.land/x/@1.1.1/a1/server.ts']\n",
    "#--------------------------------------------------------------\n",
    "#a[1],a[2]の記憶\n",
    "# @がないものの扱い（関数）：x , strの前後で分ける\n",
    "for i in https_list_a:\n",
    "    all_list = []\n",
    "    if '@' in i:\n",
    "        a  = search(i , \"@\" , \"/\")\n",
    "        print(\"はじめは\",a[1])\n",
    "        print(\"おわりは\",a[2])\n",
    "        print(\"@あり\")\n",
    "        print(\"------------------\")\n",
    "    else:\n",
    "        l = re.split('/x/|/std/',i)\n",
    "        print(\"はじめは\",l[0])\n",
    "        print(\"おわりは\",l[1])\n",
    "        print(\"@なし\")\n",
    "        print(\"------------------\")\n",
    "        \n",
    "print(all_list)\n",
    "        #if i.startswith(a[1]):\n",
    "            #if i.endswith(a[2]):\n",
    "                #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "はじめは https://deno.land/x/gokv\n",
      "おわりは /mod.ts\n",
      "@あり\n",
      "------------------\n",
      "はじめは https://deno.land/std\n",
      "おわりは /http/http_status.ts\n",
      "@あり\n",
      "------------------\n",
      "はじめは https://deno.land/std\n",
      "おわりは /http/http_status.ts\n",
      "@あり\n",
      "------------------\n",
      "はじめは https://deno.land/x\n",
      "おわりは /a1/server.ts\n",
      "@なし\n",
      "------------------\n",
      "はじめは https://deno.land/x/\n",
      "おわりは /a1/server.ts\n",
      "@あり\n",
      "------------------\n",
      "はじめは https://deno.land/x/\n",
      "おわりは /a1/server.ts\n",
      "@あり\n",
      "------------------\n",
      "[['https://deno.land/x/gokv', '/mod.ts'], ['https://deno.land/std', '/http/http_status.ts'], ['https://deno.land/std', '/http/http_status.ts'], ['https://deno.land/x', '/a1/server.ts'], ['https://deno.land/x/', '/a1/server.ts'], ['https://deno.land/x/', '/a1/server.ts']]\n",
      "                       left                 right\n",
      "0  https://deno.land/x/gokv               /mod.ts\n",
      "1     https://deno.land/std  /http/http_status.ts\n",
      "2     https://deno.land/std  /http/http_status.ts\n",
      "3       https://deno.land/x         /a1/server.ts\n",
      "4      https://deno.land/x/         /a1/server.ts\n",
      "5      https://deno.land/x/         /a1/server.ts\n",
      "                       left                 right\n",
      "0  https://deno.land/x/gokv               /mod.ts\n",
      "1     https://deno.land/std  /http/http_status.ts\n",
      "3       https://deno.land/x         /a1/server.ts\n",
      "4      https://deno.land/x/         /a1/server.ts\n",
      "                   left          right\n",
      "4  https://deno.land/x/  /a1/server.ts\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-176-96fbab4f80cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msindf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msindf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m \u001b[0msindf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msindf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msindf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[0msindf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5065\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5066\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5067\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'str'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def search_at(text, left_str, right_str):\n",
    "    global start\n",
    "    global end\n",
    "    global new_text\n",
    "    end_list = []\n",
    "    new_text_list = []\n",
    "    start_list = []\n",
    "    a= []\n",
    "    moji_find = 0\n",
    "    for line in text:\n",
    "        if line.find(left_str) == 0: # @みつけたら\n",
    "            new_text = \"\".join(new_text_list)\n",
    "            start = new_text\n",
    "            moji_find = 1\n",
    "        elif moji_find != 1: # @~/以外の文字列を追加し続ける\n",
    "            new_text_list.append(line)\n",
    "            end_list.append(line)\n",
    "        elif line.find(right_str) == 0: # /みつけたら\n",
    "            moji_find = 0\n",
    "            new_text_list.append(line)\n",
    "            end_list = []\n",
    "            end_list.append(\"/\")\n",
    "    end =  \"\".join(end_list)\n",
    "    text = \"\".join(new_text) #リストをstr形式に変換する\n",
    "    return text , start , end\n",
    "\n",
    "def search_notat(text, x1 , x2):\n",
    "    global x\n",
    "    x_find = 0\n",
    "    start_list = []\n",
    "    end_list = []\n",
    "    list_slash = text.split(\"/\")\n",
    "    for moji in list_slash:\n",
    "        if (moji == x1) or (moji == x2):\n",
    "            x_find = 1\n",
    "            start_list.append(moji)\n",
    "            moji = ''\n",
    "        elif x_find != 1:\n",
    "            start_list.append(moji + \"/\")\n",
    "        elif x_find == 1:\n",
    "            end_list.append(\"/\" + moji)\n",
    "    #start_list.append(x)\n",
    "    start = \"\".join(start_list)\n",
    "    end = \"\".join(end_list)\n",
    "    return start,end\n",
    "        \n",
    "            \n",
    "            \n",
    "#------------------------------------------------------------\n",
    "https_list_a = ['https://deno.land/x/gokv@0.0.12/mod.ts',\n",
    " 'https://deno.land/std@0.130.0/http/http_status.ts',\n",
    "'https://deno.land/std@0.140.0/http/http_status.ts',\n",
    " 'https://deno.land/x/a1/server.ts',\n",
    "'https://deno.land/x/@1.1.1/a1/server.ts',\n",
    "'https://deno.land/x/@1.1.1/a1/server.ts']\n",
    "#--------------------------------------------------------------\n",
    "#a[1],a[2]の記憶\n",
    "# @がないものの扱い（関数）：x , strの前後で分ける\n",
    "all_list = []\n",
    "for i in https_list_a:\n",
    "    if '@' in i:\n",
    "        a  = search(i , \"@\" , \"/\")\n",
    "        print(\"はじめは\",a[1])\n",
    "        print(\"おわりは\",a[2])\n",
    "        print(\"@あり\")\n",
    "        print(\"------------------\")\n",
    "        list_at = [a[1],a[2]]\n",
    "        all_list.append(list_at)\n",
    "    else:\n",
    "        l = search_notat(i,\"x\",\"std\")\n",
    "        print(\"はじめは\",l[0])\n",
    "        print(\"おわりは\",l[1])\n",
    "        print(\"@なし\")\n",
    "        print(\"------------------\")\n",
    "        list_notat = [l[0],l[1]]\n",
    "        all_list.append(list_notat)\n",
    "        \n",
    "print(all_list)\n",
    "\n",
    "df = pd.DataFrame(all_list,\n",
    "                  columns=['left','right'])\n",
    "print(df)\n",
    "df.to_csv(\"output_pd.csv\", encoding=\"shift_jis\")\n",
    "sindf = df.drop_duplicates()\n",
    "#sindf\n",
    "print(sindf)\n",
    "sindf.to_csv(\"output_sinpd.csv\", encoding=\"shift_jis\")\n",
    "\n",
    "\n",
    "print(sindf[sindf['left'].str.endswith('/')])\n",
    "\n",
    "sindf['left']=(sindf[sindf['left'].str.endswith('/')]).str[:-1]\n",
    "\n",
    "sindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://deno.land/std@0.140.0/http/http_status.ts', 'https://deno.land/std@0.130.0/http/http_status.ts', 'https://deno.land/x/a1/server.ts', 'https://deno.land/x/gokv@0.0.12/mod.ts', 'https://deno.land/x/@1.1.1/a1/server.ts'}\n"
     ]
    }
   ],
   "source": [
    "https_list_a = ['https://deno.land/x/gokv@0.0.12/mod.ts',\n",
    " 'https://deno.land/std@0.130.0/http/http_status.ts',\n",
    "'https://deno.land/std@0.140.0/http/http_status.ts',\n",
    " 'https://deno.land/x/a1/server.ts',\n",
    "'https://deno.land/x/@1.1.1/a1/server.ts',\n",
    "'https://deno.land/x/@1.1.1/a1/server.ts']\n",
    "\n",
    "\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('https://deno.land/std', '/a1/server.ts')\n"
     ]
    }
   ],
   "source": [
    "def search_notat(text, x1 , x2):\n",
    "    global x\n",
    "    x_find = 0\n",
    "    start_list = []\n",
    "    end_list = []\n",
    "    list_slash = text.split(\"/\")\n",
    "    for moji in list_slash:\n",
    "        if (moji == x1) or (moji == x2):\n",
    "            x_find = 1\n",
    "            start_list.append(moji)\n",
    "        elif x_find != 1:\n",
    "            start_list.append(moji + \"/\")\n",
    "        elif x_find == 1:\n",
    "            end_list.append(\"/\" + moji)\n",
    "    #start_list.append(x)\n",
    "    start = \"\".join(start_list)\n",
    "    end = \"\".join(end_list)\n",
    "    return start,end\n",
    "\n",
    "\n",
    "text_a = 'https://deno.land/std/a1/server.ts'\n",
    "\n",
    "a = search_notat(text_a, \"x\", \"std\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "def search_at(text, left_str, right_str):\n",
    "    global start\n",
    "    global end\n",
    "    global new_text\n",
    "    end_list = []\n",
    "    new_text_list = []\n",
    "    start_list = []\n",
    "    moji_find = 0\n",
    "    for line in text:\n",
    "        if line.find(left_str) == 0: # @みつけたら\n",
    "            new_text = \"\".join(new_text_list)\n",
    "            start = new_text\n",
    "            moji_find = 1\n",
    "        elif moji_find != 1: # @~/以外の文字列を追加し続ける\n",
    "            new_text_list.append(line)\n",
    "            end_list.append(line)\n",
    "        elif line.find(right_str) == 0: # /みつけたら\n",
    "            moji_find = 0\n",
    "            new_text_list.append(line)\n",
    "            end_list = []\n",
    "            end_list.append(\"/\")\n",
    "    end =  \"\".join(end_list)\n",
    "    text = \"\".join(new_text) #リストをstr形式に変換する\n",
    "    return text , start , end\n",
    "def search_notat(text, x1 , x2):\n",
    "    global x\n",
    "    x_find = 0\n",
    "    start_list = []\n",
    "    end_list = []\n",
    "    list_slash = text.split(\"/\")\n",
    "    for moji in list_slash:\n",
    "        if moji == x1: #xをみつけたとき\n",
    "            x_find = 1\n",
    "            x = moji\n",
    "        elif moji == x2:#stdをみつけたとき\n",
    "            x_find = 1\n",
    "            x = moji\n",
    "        elif x_find != 1 : #xかstdをみつけるまで\n",
    "            start_list.append(moji + \"/\")\n",
    "            x = ''\n",
    "        elif x_find == 1: #xかstdをみつけたあと\n",
    "            end_list.append(\"/\" + moji)\n",
    "    start_list.append(x)\n",
    "    start = \"\".join(start_list)\n",
    "    end = \"\".join(end_list)\n",
    "    return start,end\n",
    "#------------------------------------------------------------\n",
    "https_list_a = ['https://deno.land/x/gokv@0.0.12/mod.ts',\n",
    "'https://deno.land/std@0.130.0/http/http_status.ts',\n",
    "'https://deno.land/std@0.140.0/http/http_status.ts',\n",
    "'https://deno.land/x/a1/server.ts',\n",
    "'https://deno.land/x/@1.1.1/a1/server.ts',\n",
    "'https://deno.land/x/@1.1.1/a1/server.ts']\n",
    "#--------------------------------------------------------------\n",
    "#a[1],a[2]の記憶\n",
    "df_https = pd.read_csv('deno.land_dependencies_import_https.csv')\n",
    "https_list_all = df_https['import'].values.tolist()\n",
    "\n",
    "\n",
    "all_list = []\n",
    "for i in https_list_all:\n",
    "    if '@' in i:\n",
    "        a  = search_at(i , \"@\" , \"/\")\n",
    "        if a[1][len(a[1])-1] == \"/\":\n",
    "            list1 = list(a[1])\n",
    "            list1.pop()\n",
    "            left = \"\".join(list1)\n",
    "        else:\n",
    "            left = a[1]\n",
    "        if a[2][0] != \"/\":\n",
    "            list2 = list(a[2])\n",
    "            list2.insert(0, \"/\")\n",
    "            right = \"\".join(list2)\n",
    "        else:\n",
    "            right = a[2]\n",
    "        #print(\"@あり\")\n",
    "        #print(\"------------------\")\n",
    "        list_at = [left,right]\n",
    "        all_list.append(list_at)\n",
    "    else:\n",
    "        l = search_notat(i,\"x\",\"std\")\n",
    "        #l = re.split('/x/|/std/',i)\n",
    "        #print(\"はじめは\",l[0])\n",
    "        #print(\"おわりは\",l[1])\n",
    "        #print(\"@なし\")\n",
    "        #print(\"------------------\")\n",
    "        all_list.append(l)\n",
    "#print(all_list)\n",
    "df = pd.DataFrame(all_list,\n",
    "                  columns=['left','right'])\n",
    "#print(df)\n",
    "df.to_csv(\"output_pd.csv\", encoding=\"shift_jis\")\n",
    "sindf = df.drop_duplicates()\n",
    "sindf\n",
    "#print(sindf)\n",
    "sindf.to_csv(\"output_sinpd.csv\", encoding=\"shift_jis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_right = sindf['right'].tolist()\n",
    "list_of_left = sindf['left'].tolist()\n",
    "\n",
    "#print(list_of_right)\n",
    "#print(list_of_left)\n",
    "\n",
    "\n",
    "df_https = pd.read_csv('deno.land_dependencies_import_https.csv')\n",
    "list_ex = []\n",
    "\n",
    "\n",
    "for i in range(len(list_of_right)):\n",
    "    module_name = list_of_left[i] + list_of_right[i] \n",
    "    if module_name.endswith('/'):\n",
    "        module_name = module_name[:-1]\n",
    "    if len(list_of_right[i]) == 0:\n",
    "        df_ax = df_https[df_https['import'].str.startswith(list_of_left[i])]\n",
    "        list_ex.append([module_name, df_ax['import'].values.tolist()])\n",
    "    else:\n",
    "        df_ex = df_https[((df_https['import'].str.endswith(list_of_right[i])) & (df_https['import'].str.startswith(list_of_left[i])))]\n",
    "        list_ex.append([module_name, df_ex['import'].values.tolist()])\n",
    "    \n",
    "#print(list_ex)    \n",
    "\n",
    "\n",
    "df_com = pd.DataFrame(list_ex,\n",
    "                  columns=['module name','all module'])\n",
    "df_com.to_csv(\"output_pd_com.csv\", encoding=\"shift_jis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>original</th>\n",
       "      <th>import</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>488</td>\n",
       "      <td>https://deno.land/x/adka@0.1.5/deps.ts</td>\n",
       "      <td>https://raw.githubusercontent.com/apiel/logol/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                original  \\\n",
       "62         488  https://deno.land/x/adka@0.1.5/deps.ts   \n",
       "\n",
       "                                               import  \n",
       "62  https://raw.githubusercontent.com/apiel/logol/...  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_https[df_https['import'].str.contains('raw.githubusercontent.com/apiel/caller')].to_csv(\"a.csv\", encoding=\"shift_jis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_https[df_https['import'].str.contains('esm.sh/pako')].to_csv(\"a.csv\", encoding=\"shift_jis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_https[df_https['import'].str.contains('cdn.skypack')].to_csv(\"a.csv\", encoding=\"shift_jis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "def search_at(text, left_str, right_str):\n",
    "    global start\n",
    "    global end\n",
    "    global new_text\n",
    "    end_list = []\n",
    "    new_text_list = []\n",
    "    start_list = []\n",
    "    moji_find = 0\n",
    "    for line in text:\n",
    "        if line.find(left_str) == 0: # @みつけたら\n",
    "            new_text = \"\".join(new_text_list)\n",
    "            start = new_text\n",
    "            moji_find = 1\n",
    "        elif moji_find != 1: # @~/以外の文字列を追加し続ける\n",
    "            new_text_list.append(line)\n",
    "            end_list.append(line)\n",
    "        elif line.find(right_str) == 0: # /みつけたら\n",
    "            moji_find = 0\n",
    "            new_text_list.append(line)\n",
    "            end_list = []\n",
    "            end_list.append(\"/\")\n",
    "    end =  \"\".join(end_list)\n",
    "    text = \"\".join(new_text) #リストをstr形式に変換する\n",
    "    return text , start , end\n",
    "def search_notat(text, x1 , x2):\n",
    "    global x\n",
    "    x_find = 0\n",
    "    start_list = []\n",
    "    end_list = []\n",
    "    list_slash = text.split(\"/\")\n",
    "    for moji in list_slash:\n",
    "        if moji == x1: #xをみつけたとき\n",
    "            x_find = 1\n",
    "            x = moji\n",
    "        elif moji == x2:#stdをみつけたとき\n",
    "            x_find = 1\n",
    "            x = moji\n",
    "        elif x_find != 1 : #xかstdをみつけるまで\n",
    "            start_list.append(moji + \"/\")\n",
    "            x = ''\n",
    "        elif x_find == 1: #xかstdをみつけたあと\n",
    "            end_list.append(\"/\" + moji)\n",
    "    start_list.append(x)\n",
    "    start = \"\".join(start_list)\n",
    "    end = \"\".join(end_list)\n",
    "    return start,end\n",
    "#------------------------------------------------------------\n",
    "https_list_a = ['https://deno.land/x/gokv@0.0.12/mod.ts',\n",
    "'https://deno.land/std@0.130.0/http/http_status.ts',\n",
    "'https://deno.land/std@0.140.0/http/http_status.ts',\n",
    "'https://deno.land/x/a1/server.ts',\n",
    "'https://deno.land/x/@1.1.1/a1/server.ts',\n",
    "'https://deno.land/x/@1.1.1/a1/server.ts']\n",
    "#--------------------------------------------------------------\n",
    "#a[1],a[2]の記憶\n",
    "df_https = pd.read_csv('deno.land_dependencies_import_https.csv')\n",
    "https_list_all = df_https['import'].values.tolist()\n",
    "\n",
    "\n",
    "all_list = []\n",
    "for i in https_list_all:\n",
    "    if '@' in i:\n",
    "        a  = search_at(i , \"@\" , \"/\")\n",
    "        if a[1][len(a[1])-1] == \"/\":\n",
    "            list1 = list(a[1])\n",
    "            list1.pop()\n",
    "            left = \"\".join(list1)\n",
    "        else:\n",
    "            left = a[1]\n",
    "        if a[2][0] != \"/\":\n",
    "            list2 = list(a[2])\n",
    "            list2.insert(0, \"/\")\n",
    "            right = \"\".join(list2)\n",
    "        else:\n",
    "            right = a[2]\n",
    "        list_at = [left,right]\n",
    "        all_list.append(list_at)\n",
    "    else:\n",
    "        l = search_notat(i,\"x\",\"std\")\n",
    "\n",
    "        all_list.append(l)\n",
    "#print(all_list)\n",
    "df = pd.DataFrame(all_list,\n",
    "                  columns=['left','right'])\n",
    "#print(df)\n",
    "df.to_csv(\"output_pd.csv\", encoding=\"shift_jis\")\n",
    "sindf = df.drop_duplicates()\n",
    "sindf\n",
    "#print(sindf)\n",
    "sindf.to_csv(\"output_sinpd.csv\", encoding=\"shift_jis\")\n",
    "\n",
    "\n",
    "\n",
    "list_of_right = sindf['right'].tolist()\n",
    "list_of_left = sindf['left'].tolist()\n",
    "\n",
    "#print(list_of_right)\n",
    "#print(list_of_left)\n",
    "\n",
    "\n",
    "df_https = pd.read_csv('deno.land_dependencies_import_https.csv')\n",
    "list_ex = []\n",
    "\n",
    "\n",
    "for i in range(len(list_of_right)):\n",
    "    module_name = list_of_left[i] + list_of_right[i] \n",
    "    if list_of_left[i] == list_of_right[i]:\n",
    "        module_name = list_of_right[i]\n",
    "    if module_name.endswith('/'):\n",
    "        module_name = module_name[:-1]\n",
    "    #if len(list_of_right[i]) == 0:\n",
    "        #df_ax = df_https[df_https['import'].str.startswith(list_of_left[i])]\n",
    "        #list_ex.append([module_name, df_ax['import'].values.tolist()])\n",
    "    df_ex = df_https[((df_https['import'].str.endswith(list_of_right[i])) & (df_https['import'].str.startswith(list_of_left[i])))]\n",
    "    list_ex.append([module_name, df_ex['import'].values.tolist()])\n",
    "    \n",
    "#print(list_ex)    \n",
    "\n",
    "\n",
    "df_com = pd.DataFrame(list_ex,\n",
    "                  columns=['module name','all module'])\n",
    "df_com.to_csv(\"output_pd_com.csv\", encoding=\"shift_jis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import collections\n",
    "import matplotlib.pylab as plt\n",
    "import pprint\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "#result_foldaname = \"C:/Users/hayato/Desktop/kenkyu/kekka\"\n",
    "#os.makedirs(result_foldaname,exist_ok = True)\n",
    "#csvファイルを辞書型に変換--------------------------------------------------------------\n",
    "#with open('C:/Users/hayato/ScorecardsResearch-22B/deno/deno.land_dependencies_all.csv', mode='r') as inp:\n",
    "#    reader = csv.reader(inp)\n",
    "#    dict_from_csv = {rows[0]:rows[1] for rows in reader}\n",
    "#pprint.pprint(dict_from_csv)\n",
    "#-------------------------------------------------------------------------------------\n",
    "#https:から始まるリストを作成----------------------------------------------------------\n",
    "#https_list = []\n",
    "#for i in dict_from_csv.values():\n",
    "#    if i.startswith('https:'):\n",
    "#        https_list.append(i)\n",
    "#pprint.pprint(https_list)\n",
    "#print(\"https:から始まるものの数は \" , len(https_list))\n",
    "#https:から始まるもののリストを作成-------------------------------------------------------\n",
    "df = pd.read_csv('deno.land_dependencies_import_https_all.csv',encoding = \"latin-1\")#originalとimportの羅列された元のcsvファイルをdfという変数に格納する\n",
    "df_af=df[df['import'].str.contains('https://')]\n",
    "df_af.to_csv('deno.land_dependencies_import_https_all.csv')\n",
    "https_list = df_af['import'].values.tolist()\n",
    "#----------------------------------------------------------------------------------------\n",
    "d = {}\n",
    "data_list = []\n",
    "data_list2 = []\n",
    "for i in https_list:\n",
    "    version = re.findall(r'\\d*\\.\\d*\\.\\d*', i) #正規表現\n",
    "    ver = \"\".join(version) #文字列に変換\n",
    "    module = i.replace(ver, '') #モジュール名（バージョンを抜いたもの）\n",
    "    if \"/@\" in module:\n",
    "        full = module.replace(\"/@\", \"\")\n",
    "    elif \"/@v\" in module:\n",
    "        full = module.replace(\"/@v\", \"\")\n",
    "    elif \"@\" in module:\n",
    "        full = module.replace(\"@\", \"\")\n",
    "    elif \"@v\" in module:\n",
    "        full = module.replace(\"@v\", \"\")\n",
    "    else:\n",
    "        full = module\n",
    "    if ver != \"\": #バージョンありの場合\n",
    "        count = 1\n",
    "        if full in data_list: #ありの数を変更\n",
    "            count_full = data_list.count(full) # versionありで出たことがある場合\n",
    "            count = count + count_full\n",
    "            d[full][\"あり\"] = count\n",
    "            d[full][\"バージョン\"].append(ver)\n",
    "        elif full in data_list2 and full not in data_list: #versionなしで出たが、versionありでは出ていない場合\n",
    "            d[full][\"あり\"] = count #ありの数を1に\n",
    "            d[full][\"バージョン\"].append(ver)\n",
    "        else:\n",
    "            d[full] = {'あり':count, 'なし':0, 'バージョン':[ver]} #今までに同名のモジュールがでていないばあい、ありの数を1に\n",
    "        data_list.append(full)\n",
    "    else:\n",
    "        count2 = 1\n",
    "        if full in data_list2:#versionなしで出たことがある場合\n",
    "            count_full2 = data_list2.count(full)\n",
    "            count2 = count2 + count_full2\n",
    "            d[full][\"なし\"] = count2\n",
    "        elif  full in data_list not in data_list2:#バージョンありで出たが、バージョンなしでは出ていない場合\n",
    "            d[full][\"なし\"] = count2\n",
    "        else:#今までに同名のモジュールが出ていない場合\n",
    "            d[full] = {\"あり\":0 ,\"なし\":count2,\"バージョン\":[]}\n",
    "        data_list2.append(full)\n",
    "#pprint.pprint(d)\n",
    "with open('kekka_all.csv', 'w') as f:\n",
    "    writer = csv.writer(f,lineterminator=\"\\n\")\n",
    "    writer.writerow(['module_name', 'version', 'no_version',\"sum\",\"v\"])\n",
    "    for k, v in d.items():\n",
    "        gokei = v[\"あり\"]+v[\"なし\"]\n",
    "        s = pd.Series(v[\"バージョン\"])\n",
    "        w = s.value_counts().to_dict()\n",
    "        #ver = collections.Counter(v[\"バージョン\"])\n",
    "        writer.writerow([k, v[\"あり\"],v[\"なし\"],gokei,w])\n",
    "        \n",
    "        \n",
    "a = []\n",
    "if d[full][\"なし\"] != 0:\n",
    "    a = list(d.keys)\n",
    "#print(list_a)\n",
    "#s = pd.Series(a)\n",
    "#s.to_csv(\"no_version.csv\", encoding=\"shift_jis\")\n",
    "\n",
    "'''with open(\"no_version.csv\", 'wt') as f:\n",
    "    writer = csv.writer(f,lineterminator=\"\\n\")\n",
    "    writer = csv.writer(f)\n",
    "    for ele in a:\n",
    "        writer.writerow([ele])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1.0.0': 1}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "s = pd.Series(v[\"バージョン\"])\n",
    "d = s.value_counts().to_dict()\n",
    "print(d)\n",
    "print(type(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "df_https = pd.read_csv('deno.land_dependencies_import_https_all.csv')\n",
    "https_list_all = df_https['import'].values.tolist()\n",
    "\n",
    "version_ari = []\n",
    "version_nashi = []\n",
    "\n",
    "for i in https_list_all:\n",
    "    \n",
    "    if re.search(r'\\d*\\.\\d*\\.\\d*', i):\n",
    "        version_ari.append(i)\n",
    "    else:\n",
    "        version_nashi.append(i)\n",
    "        \n",
    "#version_nashi\n",
    "#s = pd.Series(version_nashi)\n",
    "#s.to_csv(\"no_version.csv\", encoding=\"shift_jis\", index = False)\n",
    "list_num=[]\n",
    "list_no_num=[]\n",
    "for q in version_nashi:\n",
    "    if re.search(r'\\d',q):\n",
    "        list_num.append(q)\n",
    "    else:\n",
    "        list_no_num.append(q)\n",
    "        \n",
    "print(len(list_num))\n",
    "\n",
    "#s = pd.Series(version_nashi)\n",
    "#s.to_csv(\"no_version.csv\", encoding=\"shift_jis\", index = False)\n",
    "\n",
    "k = pd.DataFrame(list_num,\n",
    "                  columns=['module name'])\n",
    "k.to_csv(\"list_num.csv\", encoding=\"shift_jis\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "alist=[1,2,32,33,35,67,68,77,78,79,80,83,84,89,90,92,93,94,95,96,99,100,109,110,111,113,114,118,122,123,124,125,126,127,130,131,132,133,134,135,136,137,138,139,142,187,190,195,196,197,198,207,208,209,210,211,213,220,221,231,232,233,235,236,237,238,239,240,247,249,252,253,254,255,256,257,258,263,269,275,278,279,280,282,319,338,340,350,351,353,354,361,362,364,367,371,385,386,389,390,391,392,402,409,410,416,466,469,472]\n",
    "blist=[9,10,11,12,13,14,15,16,17,18,19,20,21,22,29,30,31,34,36,61,62,63,64,65,66,81,82,85,86,115,117,128,129,141,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,169,170,180,181,182,183,185,186,191,192,193,194,199,200,201,202,203,204,205,212,214,215,216,217,218,219,224,225,226,227,228,234,250,251,262,264,265,266,267,270,271,272,274,277,283,285,287,288,289,290,291,292,293,294,295,296,297,301,302,305,306,307,308,310,311,312,314,315,316,317,318,333,334,335,337,342,345,346,347,348,349,360,369,372,373,374,387]\n",
    "clist=[23,24,25,26,27,28,69,70,71,72,73,74,75,76,87,88,91,97,98,103,104,105,106,107,108,112,116,119,120,121,143,168,171,172,173,174,175,176,177,178,179,206,222,223,230,242,243,244,245,248,260,261,268,273,284,320,321,322,323,324,325,326,327,328,329,330,331,339,352,375,376,377,378,379,380,382,383,384,393,394,395,397,398,407,408,439,440,447,448,460,461,462,463,463,467,473,375,476,477,478,479]\n",
    "err_list=[3,4,5,6,7,8,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,101,102,140,167,184,189,229,246, 281, 286, 298, 299, 300, 303, 304, 309, 313, 332, 336, 343, 344, 355, 356, 357, 358, 359, 363, 365, 366, 368, 370, 381, 388, 403, 204, 405, 406, 412, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 435, 436, 437, 438, 449, 450, 451, 452, 453, 454, 455, 356, 457, 458, 459, 481, 482]\n",
    "\n",
    "a_l = []\n",
    "b_l = []\n",
    "c_l = []\n",
    "e_l = []\n",
    "for i in err_list:\n",
    "    e_l.append(list_num[i-1])\n",
    "    \n",
    "for i in alist:\n",
    "    a_l.append(list_num[i-1])\n",
    "\n",
    "for i in blist:\n",
    "    b_l.append(list_num[i-1])\n",
    "    \n",
    "for i in clist:\n",
    "    c_l.append(list_num[i-1])\n",
    "    \n",
    "k = pd.DataFrame(e_l,\n",
    "                  columns=['module name'])\n",
    "k.to_csv(\"list_e.csv\", encoding=\"shift_jis\", index = False)\n",
    "\n",
    "k = pd.DataFrame(a_l,\n",
    "                  columns=['module name'])\n",
    "k.to_csv(\"list_a.csv\", encoding=\"shift_jis\", index = False)\n",
    "\n",
    "k = pd.DataFrame(b_l,\n",
    "                  columns=['module name'])\n",
    "k.to_csv(\"list_b.csv\", encoding=\"shift_jis\", index = False)\n",
    "\n",
    "k = pd.DataFrame(c_l,\n",
    "                  columns=['module name'])\n",
    "k.to_csv(\"list_c.csv\", encoding=\"shift_jis\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d0d6c054b472>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'state'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(df.drop('state', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-30072da63653>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkekka_list2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkekka_list2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkekka_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[0mkekka_list2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "df = pd.read_csv('deno.land_dependencies_import_https_all.csv',encoding = \"latin-1\")\n",
    "original_list = df['original'].values.tolist()\n",
    "all_list = df.values.tolist()\n",
    "\n",
    "kekka_list = []\n",
    "kekka_list2 = []\n",
    "count_ver_ari = 0\n",
    "count_ver_nashi = 0\n",
    "original_list_af = []\n",
    "for i in range(len(original_list)):\n",
    "    version = re.findall(r'\\d*\\.\\d*\\.\\d*', original_list[i])#セマンティックバージョンを発見する\n",
    "    v1 = \"\".join(version) #バージョン部分を文字列に変換\n",
    "    module = original_list[i].replace(v1, '')#数字を削除\n",
    "    if \"/@\" in module:\n",
    "        full = module.replace(\"/@\", \"\")#バージョン定義のされている箇所をreplaceで削除する。\n",
    "    elif \"/@v\" in module:\n",
    "        full = module.replace(\"/@v\", \"\")\n",
    "    elif \"@\" in module:\n",
    "        full = module.replace(\"@\", \"\")\n",
    "    elif \"@v\" in module:\n",
    "        full = module.replace(\"@v\", \"\")\n",
    "    else:\n",
    "        full = module#バージョンが見つからなかった場合、そのままのモジュール名を採用（バージョン無し扱い）\n",
    " \n",
    "    original_list_af.append(full)#originaL_list_afというファイルにバージョン無しのモジュール名を格納する。\n",
    "    all_list[i][2] = full\n",
    "    \n",
    "    \n",
    "\n",
    "for i in range(len(original_list)):\n",
    "    ex_list = []\n",
    "    kari_list =[]\n",
    "    result = []\n",
    "    count_ver_ari = 0\n",
    "    count_ver_nashi = 0\n",
    "    for j in range(len(original_list)):\n",
    "        if original_list_af[i] == all_list[j][2]:\n",
    "            ex_list.append(all_list[j][3])\n",
    "    \n",
    "    for n in ex_list:\n",
    "        version = re.findall(r'\\d*\\.\\d*\\.\\d*', n)\n",
    "        v1 = \"\".join(version) #文字列に変換\n",
    "            \n",
    "        if v1 != \"\":\n",
    "            count_ver_ari += 1 \n",
    "        else:\n",
    "            count_ver_nashi += 1\n",
    "            \n",
    "    sum_ = count_ver_ari + count_ver_nashi\n",
    "    per_ver_ari = count_ver_ari / sum_\n",
    "    per_ver_nashi = count_ver_nashi / sum_\n",
    "    \n",
    "    str_per_ari = '{:.1%}'.format(per_ver_ari)\n",
    "    str_per_nashi = '{:.1%}'.format(per_ver_nashi)\n",
    "    \n",
    "        \n",
    "\n",
    "    kari_list.append(original_list_af[i])\n",
    "    kari_list.append(count_ver_ari)\n",
    "    kari_list.append(str_per_ari)\n",
    "    kari_list.append(count_ver_nashi)\n",
    "    kari_list.append(str_per_nashi)\n",
    "    kari_list.append(sum_)\n",
    "    kari_list.append(ex_list)\n",
    "    kari_list.append(result)\n",
    "    kekka_list.append(kari_list)\n",
    "    if len(ex_list) > 1:      \n",
    "        kekka_list2.append(kari_list)\n",
    "#module名,versionあり数,versionなし数,合計,合計に対するversion定義割合,importしているモジュール\n",
    "\n",
    "\n",
    "new_list = []\n",
    "for i in range(len(kekka_list2)):\n",
    "    for j in range(len(kekka_list2)):\n",
    "        if j != i:\n",
    "            x = set(kekka_list2[i][6]) & set(kekka_list2[j][6])\n",
    "            kumi_list=[]\n",
    "            if len(x) > 1:\n",
    "                kumi_list.append(x)\n",
    "                kumi_list.append(kekka_list2[j])\n",
    "                new_list[i].append(kumi_list[i])\n",
    "                \n",
    "        \n",
    "k = pd.DataFrame(kekka_list,\n",
    "                  columns=['module name','ver_ari','per_ver_ari','ver_nashi','per_ver_nashi','sum','all','comb'])\n",
    "k.to_csv(\"output_pd_original.csv\", encoding=\"shift_jis\", index = False)\n",
    "\n",
    "#k = pd.DataFrame(kekka_list2,\n",
    "#                  columns=['module name','all','combi'])\n",
    "#k.to_csv(\"output_pd_2ijou.csv\", encoding=\"shift_jis\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['b', 'e']\n",
      "1\n",
      "['d', 'h']\n",
      "2\n",
      "['b', 'e']\n",
      "3\n",
      "['d', 'h']\n",
      "4\n",
      "['h']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['a', ['b', 'e']],\n",
       " ['c', ['d', 'h']],\n",
       " ['a', ['b', 'e']],\n",
       " ['c', ['d', 'h']],\n",
       " ['g', ['h']]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_list=['a','c','a','c','g']\n",
    "b_list=[['a','b'],['c','d'],['a','e'],['c','h'],['g','h']]\n",
    "\n",
    "\n",
    "kekka_list = []\n",
    "for i in range(len(a_list)):\n",
    "    ex_list = []\n",
    "    kari_list =[]\n",
    "    for j in range(len(a_list)):\n",
    "        if a_list[i] == b_list[j][0]:\n",
    "            ex_list.append(b_list[j][1])\n",
    "    print(ex_list)\n",
    "    kari_list.append(a_list[i])\n",
    "    kari_list.append(ex_list)\n",
    "    kekka_list.append(kari_list)\n",
    "            \n",
    "kekka_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#作った全てのリストから、'module_name部分の重複を取り除いてjunk.csvというファイルにまとめるプログラム\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output_pd_original.csv',encoding = \"latin-1\")\n",
    "\n",
    "a_df = df[~df['module name'].duplicated()]\n",
    "\n",
    "a_df.to_csv(\"junk.csv\", encoding=\"shift_jis\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://deno.land/std@0.99.0/fmt/colors.ts']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('junk.csv',encoding = \"latin-1\")\n",
    "original_list = df['module name'].values.tolist()\n",
    "all_list = df['all'].values.tolist()\n",
    "\n",
    "print(all_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 164, 'https://deno.land/x/abcv/vendor/https/deno.land/std/http/cookie.ts', 'https://deno.land/std@0.99.0/http/cookie.ts', nan]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3, 4]\n",
    "l2 = [3, 4, 5, 6]\n",
    "s1 = set(l1)\n",
    "s2 = set(l2)\n",
    "print(s1 & s2)  #=> {3, 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Apple'], ['Orange'], ['Grape'], ['Apple', 'Orange'], ['Apple', 'Grape'], ['Orange', 'Grape'], ['Apple', 'Orange', 'Grape']]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "lis=['Apple','Orange','Grape']\n",
    "result = []\n",
    "for n in range(1,len(lis)+1):\n",
    "\tfor conb in itertools.combinations(lis, n):\n",
    "\t    result.append(list(conb)) #タプルをリスト型に変換\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
